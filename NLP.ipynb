{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q84Rhsk6FOWx"
   },
   "source": [
    "# Question 2: Natural Language Processing\n",
    "\n",
    "### Total Points: 46\n",
    "\n",
    "### Background:\n",
    "In this question you will fit a set of models with the goal of predicting which text messages (SMS) are spam and which are not. \n",
    "\n",
    "You will be experimenting with a variety of different embedding algorithms followed by Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mbhj24iFMEJ"
   },
   "source": [
    "## Data Loading, Exploratory Data Analysis and Vocabulary Creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zDe_6kaRCfWs"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from typing import List\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z-YrzSxEC2Hq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sms_spam (C:\\Users\\Dell\\.cache\\huggingface\\datasets\\sms_spam\\plain_text\\1.0.0\\53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40d1675a28245c5aee1038f0748f75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sms_spam\")\n",
    "texts = [x[\"sms\"] for x in dataset[\"train\"]]\n",
    "labels = [x[\"label\"] for x in dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X523_TcvC_4S"
   },
   "outputs": [],
   "source": [
    "X_train, X_m, y_train, y_m = train_test_split(texts, labels, test_size=0.3, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_m, y_m, test_size = 0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4-MHbxEmE2Hr"
   },
   "outputs": [],
   "source": [
    "def tokenize(x:str) -> List[str]:\n",
    "  \"\"\"\n",
    "  Takes string as an input and returns a list of tokens. This just splits the string x on whitespace\n",
    "  \"\"\"\n",
    "  return x.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWyKRONCVFYD"
   },
   "source": [
    "## [ 5 points ] Q2.a: Determine / Answer the following: \n",
    "\n",
    "*   How many datapoints are in the training set?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datapoints in the training set is 3901\n"
     ]
    }
   ],
   "source": [
    "datapoints = len(X_train)\n",
    "print(f'The number of datapoints in the training set is ' + str(datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   How many total tokens are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datapoints in the training set is 87535\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum([len(tokens) for tokens in [tokenize(text) for text in texts]])\n",
    "print(f'The number of datapoints in the training set is ' + str(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   What % of training examples are spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datapoints in the training set is 12.950000000000001%\n"
     ]
    }
   ],
   "source": [
    "spam_training = round(sum(y_train)/len(y_train), 4)*100\n",
    "print(f'The number of datapoints in the training set is ' + str(spam_training) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   What would the accuracy be if for every datapoint we predict it is spam? (explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_spam = sum(y_test)/len(y_test)\n",
    "accuracy = round(test_spam, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we were to predict spam for every datapoint, then the accuracy of our model would be 16.37% because that`s the percentage of spam datapoints in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "print('If we were to predict spam for every datapoint, then the accuracy of our model would be ' + str(accuracy*100) +\n",
    "      '% because that`s the percentage of spam datapoints in the test dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS6ufPCtGwQp"
   },
   "source": [
    "## [4 points ] Q2.b Vocabulary and Frequencies  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyKQsSieGzDk"
   },
   "source": [
    "Implement the functions `create_count_dict`, and `create_vocab` below that take as input a dataset (list of untokenized strings - one per datapoint), and an integer representing the desired vocabulary size, and returns the vocabulary as a `dict` whose keys are tokens and whose values are indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ACy4z8-oGSEp"
   },
   "outputs": [],
   "source": [
    "def create_count_dict(dataset:List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Given a dataset, returns a dictionary of token counts with tokens being keys, and counts being values\n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    tokens = []\n",
    "    for text in dataset:\n",
    "        tokens = tokens + tokenize(text)\n",
    "\n",
    "    for token in set(tokens):\n",
    "        count_dict[token] = tokens.count(token)\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "def create_vocab(dataset:List[str], vocab_size:int) -> dict:\n",
    "    \"\"\"\n",
    "    Given a dataset, returns a dictionary with tokens as keys and indices as values.\n",
    "\n",
    "    {\"token_1\": 1, \"token_2\": 2, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: \n",
    "    # 1. Determine the count of all words\n",
    "    count_dict = create_count_dict(dataset)\n",
    "    \n",
    "    # 2. Keep the top <vocab_size> tokens\n",
    "    top_tokens = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    \n",
    "    # 3. Create the token to index dict and fill it with just the top tokens\n",
    "    vocab_to_index_dict = {}\n",
    "    \n",
    "    i = 0\n",
    "    for token, counts in top_tokens:\n",
    "        vocab_to_index_dict[token] = i\n",
    "        i += 1\n",
    "\n",
    "    return vocab_to_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZWgGXqOtnXaU"
   },
   "outputs": [],
   "source": [
    "# Helper test to at least ensure the constructed vocab is the right size\n",
    "for i in np.random.randint(low = 100, high = 500, size = 5):\n",
    "    assert(len(create_vocab(X_train, vocab_size = i)) == i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CawjqIMXd3X3"
   },
   "outputs": [],
   "source": [
    "# Create and save your vocab\n",
    "student_created_count_dict = create_count_dict(X_train)\n",
    "student_created_vocab = create_vocab(X_train, vocab_size = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx3Q8FkdrDpJ"
   },
   "source": [
    "## [4 points] Q2.c Token Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uns15d63rJS1"
   },
   "source": [
    "### Q2.c.a  Is the distribution of token counts below a relatively heavy-tailed distribution?  Would using only the top 30 tokens be sufficient for most applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of token counts has a heavy (slowly decaying) tail, using 30 words probably won't be sufficient for most applications, especially because most of the top tokens are stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9F3Sc2vtrBCa"
   },
   "outputs": [],
   "source": [
    "def visualize_top_token_distribution(count_dict, num_tokens_to_plot):\n",
    "    \"\"\"\n",
    "    Display a barplot of counts of top <num_tokens_to_plot> tokens in the <count_dict>\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data={\"token\": [key for key, _ in count_dict.items()], \"count\": [value for _, value in count_dict.items()]}).sort_values(\"count\", ascending=False)\n",
    "    df[:num_tokens_to_plot].plot(x = \"token\", y = \"count\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FFGiXJuQrfHw"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_top_token_distribution(student_created_count_dict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bzx51_xpskwS"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_top_token_distribution(student_created_count_dict, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3n6QenP-smAd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_top_token_distribution(student_created_count_dict, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4woSRLSt6bt"
   },
   "source": [
    "### Q2.c.b [2 points] Take a look at the top 10 tokens in the plot above, do you think the embeddings based on those tokens will be informative for our task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZCCCcILuG5N"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "Those words are incredibly common in any text in English (stopwords) and do not contribute to our task in terms of distinguishing between normal text and spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx96W-8PtrbV"
   },
   "source": [
    "## Q2.d [ 13 points ] Vocabulary selection and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vKCpZXNCH5do"
   },
   "outputs": [],
   "source": [
    "# NOTHING TO DO HERE, JUST SOME CONVENIENCE FUNCTIONS FOR THE FUTURE\n",
    "def index_to_token(index:int, vocabulary:dict) -> str:\n",
    "    \"\"\"\n",
    "    Given an index in the vocabulary, and the vocabulary dict with tokens as keys, and indices as values, returns the token that maps to the given index\n",
    "    \"\"\"\n",
    "    if index > len(vocabulary):\n",
    "        raise ValueError(f\"There is no index {index} in the vocabulary\")\n",
    "    inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "    return inv_vocabulary[index]\n",
    "\n",
    "def token_to_index(token:str, vocabulary:dict) -> int:\n",
    "    \"\"\"\n",
    "    Given a token, return the index of that token in the vocabulary\n",
    "    \"\"\"\n",
    "    index = vocabulary.get(token)\n",
    "    if index is None:\n",
    "        raise ValueError(f\"Token: {token} is not in the vocabulary\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "E5GFTnNtJhgV"
   },
   "outputs": [],
   "source": [
    "## NOTHING TO DO HERE\n",
    "\n",
    "# BoW with our vocab\n",
    "vectorizer_ours = CountVectorizer(max_features=500, tokenizer=tokenize, vocabulary=student_created_vocab) # Automatic BoW embeddings using your vocab\n",
    "vectorized_data_ours = vectorizer_ours.fit_transform(X_train) # Embedded data\n",
    "\n",
    "# Bow with an auto vocab\n",
    "vectorizer_auto = CountVectorizer(max_features=500) # Automatic BoW embeddings and automatic vocab\n",
    "vectorized_data_auto = vectorizer_auto.fit_transform(X_train) # Embedded data\n",
    "VOCAB = vectorizer_auto.vocabulary_ # For convenience\n",
    "\n",
    "# binary BoW\n",
    "vectorizer_bbow = CountVectorizer(max_features=500, binary=True, vocabulary=VOCAB) # Automatic binary BoW embeddings using the automatically generated vocab above\n",
    "vectorized_data_bbow = vectorizer_bbow.fit_transform(X_train) # Embedded data\n",
    "\n",
    "# TFIDF\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False) # TF-IDF transformer for BoW embeddings\n",
    "vectorized_data_tfidf_auto = tfidf_transformer.fit_transform(vectorized_data_auto) # Embedded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQFvomo9urR3"
   },
   "source": [
    "### Q2.d.a [ 2 points ] Implement the function `vocabulary_diff` below. It should return a list of tokens that are in `vocab_1` but not in `vocab_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6-CScQ3tKGPU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'a', 'u', '', '2', \"i'm\", '&lt;#&gt;', '4', '.', '\\n', 'r', '?', '', \"i'll\", \"don't\", '...', 'n', '&', \"it's\", '&amp;', 'd', 'e', '-', 'c', '1', \"can't\", 'me.', 'you.', 'now!', 'now.', 'b', \"i've\", \"that's\", '3', \"you're\", '!', 'lor.', '?\\n', 'you,', 'sorry,', ':)\\n', \"didn't\", '*', 'u.', 'urgent!', 'day.', 'k', 'y', 'later\\n', \"he's\", 'me.\\n', 'it,', '1000', 'you!', '5', \"how's\", 'ok.', 'v', 'lor...', 'you\\n', '+', 'it.', 'no.', ',', 'x', 'tomorrow\\n', 'lor...\\n', '100', 'i.ll', 'it.\\n', 'lor.\\n', 'juz', 'wif', 'g', 'makes', '500', \"we're\", 'day.\\n', 'me\\n', 'yeah,', \"there's\", \"won't\", 'saying', '.\\n', 'stay', 'home.', 'took', 'now.\\n', 'you.\\n', '&lt;decimal&gt;', 'wil', '6', 'needs']\n",
      "['pa', 'yet', 'liao', 'lt', 'gt', 'www', 'com', 'wk', '150p', '16', 'min', 'feeling', 'apply', '18', 'ah', 'plan', 'll', 'princess', 'lar', 've', 'code', '10', 'tmr', 'don', '000', 'didn', 'bus', 'kiss', 'online', 'hours', 'haha', 'amp', 'evening', 'line', 'true', 'haven', 'sir', 'month', 'problem', 'okay', 'weekend', 'leh', 'anyway', 'book', 're', 'house', '1000', 'word', 'co', 'uk', 'plz', 'afternoon', 'urgent', 'landline', 'cs', 'mail', 'baby', 'fine', 'cool', 'easy', 'rate', 'dad', 'times', 'hello', 'bored', '150ppm', 'room', 'xxx', 'aight', 'girl', 'thanx', '50', 'though', 'busy', 'else', 'orange', 'head', 'messages', 'while', 'bed', 'hour', 'comes', 'dreams', 'drive', '100', 'weekly', 'worry', 'calls', 'office', 'wife', 'mob', 'boy', 'offer']\n"
     ]
    }
   ],
   "source": [
    "# Get the differences between our vocabulary and the automatically generated one\n",
    "\n",
    "def vocabulary_diff(vocab_1:dict, vocab_2:dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given 2 vocabularies, returns all the tokens that are present in vocab_1 but not in vocab_2\n",
    "    \"\"\"\n",
    "    vocab_diff = [token for token in vocab_1.keys() if token not in vocab_2.keys()]\n",
    "    return vocab_diff\n",
    "\n",
    "\n",
    "print(vocabulary_diff(student_created_vocab,VOCAB))\n",
    "print(vocabulary_diff(VOCAB, student_created_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3gDDBttvyVW"
   },
   "source": [
    "###  Q2.d.b [3 points] Based on the differences between the two vocabularies, which do you think will represent our datapoints better? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSDcv9SYwGJj"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "I think that the VOCAB vocabulary that was created using automatic BoW embeddings represent the datapoints better. First, because in our vocabulary there are too many stopwords like 'i', 'a', 'i've', 'it's' which we can see above. Secondly, it looks like the autovocabulary managed to avoid tokenizing symbols and words followed by symbols like '.', '-', '+', 'it.\\n' or '&lt;decimal&gt;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZDa_SZbdN1tA"
   },
   "outputs": [],
   "source": [
    "# Some convenience constants\n",
    "DOCUMENT_COUNTS = np.bincount(vectorized_data_bbow.indices, minlength=vectorized_data_bbow.shape[1])\n",
    "NUM_DATAPOINTS = len(X_train)\n",
    "VOCAB = vectorizer_auto.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvRheDH8w7lF"
   },
   "source": [
    "### Q2.d.c [3 points] Implement the `idf` function below which returns the inverse document frequency of a given word as defined by:\n",
    "\n",
    "$$ IDF(w_j) = log(\\frac{N}{N_j}) + 1 $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $N$ is the number of datapoints \n",
    "\n",
    "* $N_j$ is the number of occurences of word $w_j$ in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "V-dlIRAPPzSE"
   },
   "outputs": [],
   "source": [
    "def idf(word:str, document_count:np.ndarray=DOCUMENT_COUNTS, \n",
    "        num_datapoints:int=NUM_DATAPOINTS, vocabulary:dict=VOCAB) -> float:\n",
    "    \"\"\"\n",
    "    returns the inverse document frequency of a particular word, based on the IDF definition of\n",
    "\n",
    "    IDF(w_j) = log(N / N_j) + 1\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    N = num_datapoints\n",
    "    N_j = document_count[token_to_index(word, vocabulary)]\n",
    "    return np.log(N/N_j) + 1\n",
    "    \n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7wEYAGnnQi97",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: head\n",
      "your idf: 6.4357748654504405\n",
      "calculated idf: 6.4357748654504405\n",
      "word: ah\n",
      "your idf: 6.224465771783233\n",
      "calculated idf: 6.224465771783233\n",
      "word: customer\n",
      "your idf: 5.713640148017243\n",
      "calculated idf: 5.713640148017243\n",
      "word: dont\n",
      "your idf: 4.684020730836085\n",
      "calculated idf: 4.684020730836085\n",
      "word: where\n",
      "your idf: 4.862268962242403\n",
      "calculated idf: 4.862268962242403\n",
      "word: it\n",
      "your idf: 3.2216160304603783\n",
      "calculated idf: 3.2216160304603783\n",
      "word: po\n",
      "your idf: 6.0909343791587105\n",
      "calculated idf: 6.0909343791587105\n",
      "word: weekend\n",
      "your idf: 6.177945756148341\n",
      "calculated idf: 6.177945756148341\n",
      "word: co\n",
      "your idf: 5.60542656337701\n",
      "calculated idf: 5.60542656337701\n",
      "word: sms\n",
      "your idf: 5.685469271050547\n",
      "calculated idf: 5.685469271050547\n",
      "word: could\n",
      "your idf: 5.742627684890495\n",
      "calculated idf: 5.742627684890495\n",
      "word: service\n",
      "your idf: 5.58010875539272\n",
      "calculated idf: 5.58010875539272\n",
      "word: forgot\n",
      "your idf: 6.133493993577507\n",
      "calculated idf: 6.133493993577507\n",
      "word: easy\n",
      "your idf: 6.273255935952665\n",
      "calculated idf: 6.273255935952665\n",
      "word: first\n",
      "your idf: 5.555416142802349\n",
      "calculated idf: 5.555416142802349\n"
     ]
    }
   ],
   "source": [
    "for index in np.random.randint(0,500, size=15).tolist():\n",
    "    print(f\"word: {index_to_token(index, VOCAB)}\")\n",
    "    print(f\"your idf: {idf(index_to_token(index, VOCAB))}\")\n",
    "    print(f\"calculated idf: {tfidf_transformer.idf_[index]}\")\n",
    "    assert np.isclose(tfidf_transformer.idf_[index], idf(index_to_token(index, VOCAB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIYmBJfkxg50"
   },
   "source": [
    "### Q2.d.d [2 points] Implement a binary Bag of Words embedding function that takes in a list of token and a vocab, and returns a numpy array corresponding to the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AvtjwrUyS0lT"
   },
   "outputs": [],
   "source": [
    "def binary_BoW_embedder(list_of_tokens:List[str], vocab:dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a binary Bag of Words embedding of a datapoint represented as a list of tokens, using the vocab\n",
    "    \"\"\"\n",
    "    # Hint: you may want to start with np.zeros()\n",
    "    \n",
    "    embedding = np.zeros(len(vocab))\n",
    "    \n",
    "    for token in list_of_tokens:\n",
    "        index_token = vocab[token]\n",
    "        embedding[index_token] = 1 \n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j5kCOIlVUMrM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['or', 'maybe', 'really', 'yeah', 'time', 'nothing', 'stuff', 'hello', 'wait', 'morning']\n",
      "your embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "calculated embedding: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Success!\n",
      "text: ['getting', 'weekly', 'over', 'aight', 'make', 'like', 'receive', 'beautiful', 'wish', 'birthday']\n",
      "Success!\n",
      "text: ['got', 'with', 'he', 'were', 'missed', 'hear', 'wake', 'guess', 'wake', 'bed']\n",
      "Success!\n",
      "text: ['shit', 'best', 'looking', 'per', 'easy', 'worry', 'online', 'enjoy', 'ah', 'plus']\n",
      "Success!\n",
      "text: ['going', 'money', 'sweet', 'code', 'pain', 'about', 'long', 'might', 'remember', 'before']\n",
      "Success!\n",
      "text: ['stuff', 're', 'com', 'show', 'wait', 'watching', 'trying', 'messages', 'txt', 'apply']\n",
      "Success!\n",
      "text: ['how', 'care', 'tomorrow', 'because', 'service', 'sweet', '10', 'talk', 'being', 'dat']\n",
      "Success!\n",
      "text: ['means', 'orange', 'without', 'heart', 'there', 'join', 'babe', 'girl', 'hope', 'wake']\n",
      "Success!\n",
      "text: ['world', 'miss', 'said', 'on', 'missed', 'way', 'wen', 'it', 'mail', 'hear']\n",
      "Success!\n",
      "text: ['number', 'into', 'if', 'line', 'quite', 'show', 'before', 'much', 'though', 'job']\n",
      "Success!\n",
      "text: ['mail', 'he', 'speak', 'year', 'attempt', 'fuck', 'abt', 'oh', 'boy', 'receive']\n",
      "Success!\n",
      "text: ['please', '1st', 'really', 'dunno', 'yeah', 'enough', 'wife', 'bored', 'evening', 'your']\n",
      "Success!\n",
      "text: ['care', 'text', 'give', 'told', 'nice', 'sorry', 'meet', 'xxx', 'left', 'attempt']\n",
      "Success!\n",
      "text: ['amp', 'enjoy', 'home', 'pls', 'mins', 'today', 'anything', 'be', 'this', 'book']\n",
      "Success!\n",
      "text: ['its', 'not', 'cost', 'gt', 'service', 'is', 'wish', 'apply', 'wanna', 'late']\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "for test_num in range(15):\n",
    "    sample_token_indices = np.random.randint(0,500, size=10)\n",
    "    input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
    "    input_text_as_str = \" \".join(input_text)\n",
    "    print(f\"text: {input_text}\")\n",
    "    if test_num == 0:\n",
    "        print(f\"your embedding: {binary_BoW_embedder(input_text, VOCAB)}\")\n",
    "        print(f\"calculated embedding: {vectorizer_bbow.transform([input_text_as_str]).toarray()}\")\n",
    "    assert np.isclose(binary_BoW_embedder(input_text, VOCAB), vectorizer_bbow.transform([input_text_as_str]).toarray()).all()\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QvHSEz2xzAC"
   },
   "source": [
    "### Q2.d.e [2 points] Implement a Bag of Words embedding function that takes in a list of token and a vocab, and returns a numpy array corresponding to the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RnWQOaXKaczA"
   },
   "outputs": [],
   "source": [
    "def BoW_embedder(list_of_tokens, vocab):\n",
    "    \"\"\"\n",
    "    Creates a binary Bag of Words embedding of a datapoint represented as a list of tokens, using the vocab\n",
    "    \"\"\"\n",
    "    # Hint: You may want to start with np.zeros()\n",
    "    embedding = np.zeros(len(vocab))\n",
    "    \n",
    "    for token in list_of_tokens:\n",
    "        index_token = vocab[token]\n",
    "        embedding[index_token] += 1\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zH4YTsgghcqE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['hours', 'hope', 'holiday', 'hours', 'home', 'hope', 'holiday', 'home', 'home', 'hour']\n",
      "your embedding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 3. 2. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "calculated embedding: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Success!\n",
      "text: ['hours', 'hope', 'holiday', 'hour', 'hours', 'hour', 'hours', 'hours', 'hope', 'hours']\n",
      "Success!\n",
      "text: ['hour', 'hours', 'hope', 'hope', 'hour', 'home', 'home', 'hours', 'holiday', 'hours']\n",
      "Success!\n",
      "text: ['hour', 'hour', 'hour', 'hour', 'hour', 'hope', 'home', 'hour', 'holiday', 'holiday']\n",
      "Success!\n",
      "text: ['holiday', 'holiday', 'hope', 'holiday', 'hour', 'hours', 'holiday', 'hope', 'hope', 'holiday']\n",
      "Success!\n",
      "text: ['hours', 'holiday', 'hope', 'home', 'hour', 'hope', 'holiday', 'hour', 'holiday', 'holiday']\n",
      "Success!\n",
      "text: ['home', 'hour', 'hour', 'home', 'hope', 'holiday', 'hours', 'holiday', 'holiday', 'hope']\n",
      "Success!\n",
      "text: ['holiday', 'home', 'home', 'hour', 'hours', 'holiday', 'holiday', 'hope', 'home', 'hours']\n",
      "Success!\n",
      "text: ['hour', 'home', 'hour', 'hope', 'hope', 'holiday', 'hours', 'hour', 'home', 'hope']\n",
      "Success!\n",
      "text: ['holiday', 'holiday', 'hour', 'hope', 'hours', 'hope', 'hour', 'hour', 'hope', 'hour']\n",
      "Success!\n",
      "text: ['hope', 'home', 'hope', 'hope', 'hour', 'hour', 'holiday', 'holiday', 'home', 'holiday']\n",
      "Success!\n",
      "text: ['hope', 'hour', 'holiday', 'holiday', 'home', 'home', 'hope', 'hour', 'home', 'holiday']\n",
      "Success!\n",
      "text: ['hour', 'hour', 'holiday', 'home', 'holiday', 'hour', 'hours', 'hours', 'hope', 'holiday']\n",
      "Success!\n",
      "text: ['holiday', 'hope', 'hope', 'hope', 'hour', 'holiday', 'hour', 'hope', 'holiday', 'hour']\n",
      "Success!\n",
      "text: ['hour', 'hope', 'holiday', 'hope', 'holiday', 'hours', 'home', 'home', 'home', 'hope']\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "for test_num in range(15):\n",
    "    sample_token_indices = np.random.randint(199,204, size=10)\n",
    "    input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
    "    input_text_as_str = \" \".join(input_text)\n",
    "    print(f\"text: {input_text}\")\n",
    "    if test_num == 0:\n",
    "        print(f\"your embedding: {BoW_embedder(input_text, VOCAB)}\")\n",
    "        print(f\"calculated embedding: {vectorizer_auto.transform([input_text_as_str]).toarray()}\")\n",
    "    assert np.isclose(BoW_embedder(input_text, VOCAB), vectorizer_auto.transform([input_text_as_str]).toarray()).all()\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_2Ff_JaiJ8_"
   },
   "source": [
    "## Q2.e [11 points] Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7xkV3-sphmKl"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Bkq8Wnm7jSCn"
   },
   "outputs": [],
   "source": [
    "logreg_auto = LogisticRegression(random_state=1).fit(vectorized_data_auto, y_train)\n",
    "logreg_bbow = LogisticRegression(random_state=1).fit(vectorized_data_bbow, y_train)\n",
    "logreg_tfidf = LogisticRegression(random_state=1).fit(vectorized_data_tfidf_auto, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x166D5jgzcXD"
   },
   "source": [
    "### Q2.e.a [3 points] Evaluation Metrics\n",
    "Below you can see a set of simple accuracy scores: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
    "\n",
    "In what situation would accuracy be a good evaluation metric? In what situation it wouldn't? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rBmtzT6dlmKo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All 1s; accuracy on validation data: 0.1255980861244019\n",
      " All 0s; accuracy on validation data: 0.8744019138755981\n",
      " Logistic Regression on BoW embedding; accuracy on validation data: 0.9772727272727273\n",
      " Logistic Regression on binary BoW embedding; accuracy on validation data: 0.9808612440191388\n",
      " Logistic Regression on TF-IDF embedding; accuracy on validation data: 0.972488038277512\n"
     ]
    }
   ],
   "source": [
    "print(f\" All 1s; accuracy on validation data: {accuracy_score(y_val, np.ones_like(y_val))}\")\n",
    "\n",
    "print(f\" All 0s; accuracy on validation data: {accuracy_score(y_val, np.zeros_like(y_val))}\")\n",
    "\n",
    "print(f\" Logistic Regression on BoW embedding; accuracy on validation data: {accuracy_score(y_val, logreg_auto.predict(vectorizer_auto.transform(X_val)))}\")\n",
    "\n",
    "print(f\" Logistic Regression on binary BoW embedding; accuracy on validation data: {accuracy_score(y_val, logreg_bbow.predict(vectorizer_bbow.transform(X_val)))}\")\n",
    "\n",
    "print(f\" Logistic Regression on TF-IDF embedding; accuracy on validation data: {accuracy_score(y_val, logreg_tfidf.predict(tfidf_transformer.transform(vectorizer_auto.transform(X_val))))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3gZZYLy0f37"
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy would be a good evaluation metric if we are certain that we're using the optimal value for the threshold that we use to compare the predicted probabilities. The most common default threshold is 0.5 but this often not optimal. Also, accuracy would be a useful metric if we don't care how far from the threshold our predicted probabilities are and if it makes no difference to us the type error of our model (misclassifying spam as not spam is as costly as misclassifying not spam as spam).\n",
    "\n",
    "Accuracy wouldn't be a good choice in the opposite cases of those listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OWMNrwD0hwR"
   },
   "source": [
    "### Q2.e.b [8 points] Answer the following in one-two sentences (2 points each)\n",
    "\n",
    "* What is the interpretation of coefficient $\\beta_j$ with $j > 1$ (so not the intercept) in the Logistic regression model fit on the binary BoW embeddings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB4UdDyj11C_"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "It represents how the log-odds of the SMS being spam will change by having the jth token in the datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the interpretation of coefficient $\\beta_j$ with $j > 1$ (so not the intercept) in the Logistic regression model fit on the BoW embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB4UdDyj11C_"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "It represents how the log-odds of the SMS being spam will change for each time the jth token is in the datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we were to include 1-st order interaction terms in our Logistic Regression model fit on binary BoW embeddings, what would be the interpretation of a coefficient for that interaction ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB4UdDyj11C_"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "It represents how the log-odds of the SMS being spam will change by having both tokens in the datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the above different from fitting a Logistic Regression on a combination of unigrams and bigrams (1-grams, and 2-grams)? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB4UdDyj11C_"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "Not necesarily, because for a bigram to be considered the tokens must be next to each other in the datapoint, whereas for the interaction term the tokens need to be in the datapoint but the position in relation to each other doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFZJTJ9msv_V"
   },
   "source": [
    "## Q2.f [4 points] Word Vectors\n",
    "\n",
    "Below we will be using a set of pre-trained word embeddings from the SpaCy library. For the purpose of the questions below, assume that they were trained\n",
    "using a Skip-Gram objective as described in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9vllrC6GmURD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This installs a full pipeline of tokenizer -> embedder with additional components.\n",
    "\n",
    "import spacy\n",
    "spacy_pipeline = spacy.load('en_core_web_md') # If this line produces an error,\n",
    "# you might need to re-start the runtime after running the first cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHMjDoj22TiQ"
   },
   "source": [
    "### Q2.f.a [2 points] Implement the function get_document_embedding_from_spacy below to return the document embedding as the average of all vector embeddings.\n",
    "\n",
    "You are not allowed to use `tokens.vector` in your functions, but you can check against it that your solution is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_token_indices = np.random.randint(0,500, size=10)\n",
    "input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
    "input_text_as_str = \" \".join(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Lt-s7xVZm_5t"
   },
   "outputs": [],
   "source": [
    "def get_document_embedding_from_spacy(text:str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    returns the embedding vector representing the entire text by averaging the \n",
    "    word level embeddings.\n",
    "\n",
    "    !! DO NOT USE tokens.vector directly!! \n",
    "    \"\"\"\n",
    "    tokens = spacy_pipeline(text)\n",
    "\n",
    "    # Hint: you can iterate over each token in `tokens` and access the underlying vector with .vector\n",
    "    document_embedding = np.mean([token.vector for token in tokens], axis=0)\n",
    "\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AWf-PZXlp2Do"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['thing', 'still', 'text', 'enjoy', 'here', 'then', 'dun', 'anything', 'guy', 'weekly']\n",
      "your embedding: [-1.06595896e-01  7.91089982e-02 -2.58593380e-01 -7.63885081e-02\n",
      "  1.26982003e-01  1.36778310e-01  1.83586888e-02 -2.85775721e-01\n",
      "  6.27655014e-02  2.10907006e+00 -2.63153881e-01  1.33809820e-01\n",
      "  5.81153035e-02 -9.41911116e-02 -7.38481060e-02  3.88816968e-02\n",
      " -9.82097909e-02  8.30210030e-01 -1.71062395e-01  6.49387613e-02\n",
      "  1.56051992e-02  8.96746069e-02  1.11372426e-01  4.36438993e-02\n",
      "  1.23564385e-01  1.24662995e-01  1.17879063e-02 -2.28987411e-01\n",
      "  2.32392818e-01 -1.89339310e-01 -9.12214071e-02  6.34998903e-02\n",
      " -5.53512201e-02  1.73856020e-02 -5.28411977e-02  2.86935028e-02\n",
      " -1.15404008e-02  8.54986086e-02 -9.12498981e-02 -7.86288269e-03\n",
      " -1.21571064e-01  4.63019982e-02  6.17484041e-02  1.94390006e-02\n",
      "  5.04818670e-02  2.01809645e-01  7.38304865e-04 -9.24137086e-02\n",
      "  1.46252111e-01 -1.02991499e-01 -1.42016396e-01  2.03592032e-02\n",
      "  1.30739719e-01 -3.84897031e-02  4.39146981e-02  2.80777998e-02\n",
      " -2.79606469e-02 -2.18934938e-01  1.43541709e-01 -1.24661602e-01\n",
      " -2.04804629e-01 -1.81145191e-01 -7.75483847e-02  2.30848745e-01\n",
      "  1.99162811e-01 -7.66744465e-02 -1.26187593e-01  1.39817685e-01\n",
      "  6.68797046e-02  2.80285597e-01  5.82472011e-02  5.61110079e-02\n",
      "  2.60439575e-01 -1.57350704e-01  4.69055064e-02  1.80933595e-01\n",
      "  9.23603922e-02 -2.75030080e-02 -1.40353099e-01  4.03231770e-01\n",
      "  1.44658104e-01 -1.58770829e-01 -1.78963721e-01  1.75025016e-02\n",
      "  1.50456518e-01 -8.63724500e-02  2.42081597e-01 -2.35893995e-01\n",
      "  3.42170000e-01 -2.29712985e-02 -1.84123591e-01  2.96382010e-02\n",
      " -4.52380106e-02 -8.21980182e-03  9.84809920e-02 -5.11066988e-02\n",
      " -5.37246540e-02  6.57789931e-02  1.67048424e-02  7.22029954e-02\n",
      "  1.57989468e-02 -6.29649684e-02  5.08167967e-02 -2.05135830e-02\n",
      "  2.29128286e-01 -6.37559474e-01  1.88925907e-01 -3.19631994e-02\n",
      " -7.89142996e-02  8.82833973e-02  1.16750404e-01 -3.11527312e-01\n",
      "  1.53700709e-01  5.18913381e-02  4.41691019e-02 -1.27672013e-02\n",
      " -2.93272976e-02 -1.24811009e-01 -1.07473098e-01  4.83098030e-02\n",
      "  6.61764964e-02 -1.84243679e-01  1.08620990e-02  1.10139504e-01\n",
      "  2.62492478e-01  5.51811978e-02 -1.00007199e-01 -2.81838864e-01\n",
      "  1.00885712e-01 -4.90220077e-02 -3.81788984e-02 -4.46130969e-02\n",
      "  3.34000029e-02  2.11831599e-01  1.16594002e-01  7.49420077e-02\n",
      "  3.26051004e-02  4.35209274e-02 -9.09726173e-02 -6.37352020e-02\n",
      " -1.95571208e+00  1.32486105e-01  9.53231081e-02 -1.52291004e-02\n",
      "  1.15547299e-01 -2.21491128e-01 -7.11062923e-02  2.86528729e-02\n",
      " -9.37120914e-02 -8.88989046e-02  1.70937981e-02  2.26753399e-01\n",
      "  1.25671878e-01 -8.54025930e-02 -7.85490051e-02  2.87992954e-02\n",
      " -1.99307986e-02 -1.89196095e-01 -1.49880974e-02 -6.37326017e-02\n",
      " -1.16385795e-01  3.80611047e-02 -1.20519891e-01 -4.75234985e-02\n",
      " -1.58697888e-01 -8.10488984e-02 -8.75438973e-02  9.70104933e-02\n",
      "  1.06915593e-01 -1.15292907e-01 -5.30473962e-02  8.01635012e-02\n",
      "  1.92782320e-02 -2.98622012e-01  2.58589396e-03  1.11349799e-01\n",
      " -4.63622995e-02  2.51499703e-03 -4.58507985e-02 -2.65094992e-02\n",
      " -5.65071478e-02 -1.75256968e-01 -1.71754196e-01 -8.81151762e-03\n",
      " -8.81879963e-03 -1.02337494e-01 -9.14853960e-02 -1.87836111e-01\n",
      "  2.44250894e-01 -1.47913545e-01  6.71698973e-02  5.79364002e-02\n",
      " -2.16445044e-01 -4.42338400e-02  2.02115566e-01  6.05320036e-02\n",
      " -1.05292283e-01 -1.60389155e-01  1.05685689e-01  1.44574702e-01\n",
      " -9.49916989e-02 -6.15625009e-02 -9.73447934e-02 -1.56234398e-01\n",
      "  7.02364892e-02 -2.72877999e-02  4.17655101e-03  3.05995494e-02\n",
      "  6.60447329e-02  4.81186993e-02 -2.00022012e-02 -1.07157208e-01\n",
      "  2.07515005e-02 -1.11394621e-01  8.48550163e-03  1.19025977e-02\n",
      "  1.13621950e-02 -1.49501383e-01 -3.51001501e-01  5.64678013e-02\n",
      "  1.21944621e-01 -1.11272290e-01 -4.45610052e-03  8.85736048e-02\n",
      "  1.83467977e-02 -1.52556002e-02 -3.79473381e-02  1.02317795e-01\n",
      "  1.35775715e-01 -3.69632989e-02 -2.79254317e-02  9.70505998e-02\n",
      "  8.52179900e-02  2.01948807e-01  6.10720227e-03  5.89480996e-02\n",
      "  3.87830883e-02 -3.99859138e-02 -1.39068693e-01  2.63231993e-01\n",
      " -2.48654000e-02  9.57983509e-02  3.91895184e-03  1.10756114e-01\n",
      "  4.95524928e-02 -1.43819809e-01 -1.28051996e-01 -1.09742284e-02\n",
      " -8.77822489e-02  1.28861874e-01  1.55271992e-01 -1.17266700e-01\n",
      " -4.05014977e-02  5.13070002e-02  7.21820742e-02  1.91195399e-01\n",
      "  4.10711989e-02 -7.72915930e-02 -7.08080754e-02 -9.71624777e-02\n",
      "  2.55995810e-01  2.51355767e-01  2.09300518e-02  3.95144001e-02\n",
      "  1.59459114e-01 -3.98249999e-02 -1.97775513e-01  4.47143987e-02\n",
      "  5.23810267e-01  1.64561003e-01 -1.56155974e-01 -1.52225897e-01\n",
      " -1.98557973e-02 -1.72213495e-01 -1.17671505e-01  1.61061198e-01\n",
      " -3.82662937e-02 -6.75619859e-03  1.45996017e-02  2.36894995e-01\n",
      "  1.08408108e-01 -2.75281016e-02 -1.59759820e-03 -1.58994198e-01\n",
      " -4.15528975e-02  1.12410192e-03  1.12073801e-01 -1.08727068e-02\n",
      "  5.05218022e-02  5.98489046e-02 -4.22674939e-02 -1.64209992e-01\n",
      " -2.24625394e-01 -1.67617604e-01  2.45279476e-01 -4.11821008e-02\n",
      " -4.43226956e-02 -1.27027303e-01  1.00868002e-01  1.34983405e-01]\n",
      "calculated embedding: [-1.06595896e-01  7.91089982e-02 -2.58593380e-01 -7.63885081e-02\n",
      "  1.26982003e-01  1.36778310e-01  1.83586888e-02 -2.85775721e-01\n",
      "  6.27655014e-02  2.10907006e+00 -2.63153881e-01  1.33809820e-01\n",
      "  5.81153035e-02 -9.41911116e-02 -7.38481060e-02  3.88816968e-02\n",
      " -9.82097909e-02  8.30210030e-01 -1.71062395e-01  6.49387613e-02\n",
      "  1.56051992e-02  8.96746069e-02  1.11372426e-01  4.36438993e-02\n",
      "  1.23564385e-01  1.24662995e-01  1.17879063e-02 -2.28987411e-01\n",
      "  2.32392818e-01 -1.89339310e-01 -9.12214071e-02  6.34998903e-02\n",
      " -5.53512201e-02  1.73856020e-02 -5.28411977e-02  2.86935028e-02\n",
      " -1.15404008e-02  8.54986086e-02 -9.12498981e-02 -7.86288269e-03\n",
      " -1.21571064e-01  4.63019982e-02  6.17484041e-02  1.94390006e-02\n",
      "  5.04818670e-02  2.01809645e-01  7.38304865e-04 -9.24137086e-02\n",
      "  1.46252111e-01 -1.02991499e-01 -1.42016396e-01  2.03592032e-02\n",
      "  1.30739719e-01 -3.84897031e-02  4.39146981e-02  2.80777998e-02\n",
      " -2.79606469e-02 -2.18934938e-01  1.43541709e-01 -1.24661602e-01\n",
      " -2.04804629e-01 -1.81145191e-01 -7.75483847e-02  2.30848745e-01\n",
      "  1.99162811e-01 -7.66744465e-02 -1.26187593e-01  1.39817685e-01\n",
      "  6.68797046e-02  2.80285597e-01  5.82472011e-02  5.61110079e-02\n",
      "  2.60439575e-01 -1.57350704e-01  4.69055064e-02  1.80933595e-01\n",
      "  9.23603922e-02 -2.75030080e-02 -1.40353099e-01  4.03231770e-01\n",
      "  1.44658104e-01 -1.58770829e-01 -1.78963721e-01  1.75025016e-02\n",
      "  1.50456518e-01 -8.63724500e-02  2.42081597e-01 -2.35893995e-01\n",
      "  3.42170000e-01 -2.29712985e-02 -1.84123591e-01  2.96382010e-02\n",
      " -4.52380106e-02 -8.21980182e-03  9.84809920e-02 -5.11066988e-02\n",
      " -5.37246540e-02  6.57789931e-02  1.67048424e-02  7.22029954e-02\n",
      "  1.57989468e-02 -6.29649684e-02  5.08167967e-02 -2.05135830e-02\n",
      "  2.29128286e-01 -6.37559474e-01  1.88925907e-01 -3.19631994e-02\n",
      " -7.89142996e-02  8.82833973e-02  1.16750404e-01 -3.11527312e-01\n",
      "  1.53700709e-01  5.18913381e-02  4.41691019e-02 -1.27672013e-02\n",
      " -2.93272976e-02 -1.24811009e-01 -1.07473098e-01  4.83098030e-02\n",
      "  6.61764964e-02 -1.84243679e-01  1.08620990e-02  1.10139504e-01\n",
      "  2.62492478e-01  5.51811978e-02 -1.00007199e-01 -2.81838864e-01\n",
      "  1.00885712e-01 -4.90220077e-02 -3.81788984e-02 -4.46130969e-02\n",
      "  3.34000029e-02  2.11831599e-01  1.16594002e-01  7.49420077e-02\n",
      "  3.26051004e-02  4.35209274e-02 -9.09726173e-02 -6.37352020e-02\n",
      " -1.95571208e+00  1.32486105e-01  9.53231081e-02 -1.52291004e-02\n",
      "  1.15547299e-01 -2.21491128e-01 -7.11062923e-02  2.86528729e-02\n",
      " -9.37120914e-02 -8.88989046e-02  1.70937981e-02  2.26753399e-01\n",
      "  1.25671878e-01 -8.54025930e-02 -7.85490051e-02  2.87992954e-02\n",
      " -1.99307986e-02 -1.89196095e-01 -1.49880974e-02 -6.37326017e-02\n",
      " -1.16385795e-01  3.80611047e-02 -1.20519891e-01 -4.75234985e-02\n",
      " -1.58697888e-01 -8.10488984e-02 -8.75438973e-02  9.70104933e-02\n",
      "  1.06915593e-01 -1.15292907e-01 -5.30473962e-02  8.01635012e-02\n",
      "  1.92782320e-02 -2.98622012e-01  2.58589396e-03  1.11349799e-01\n",
      " -4.63622995e-02  2.51499703e-03 -4.58507985e-02 -2.65094992e-02\n",
      " -5.65071478e-02 -1.75256968e-01 -1.71754196e-01 -8.81151762e-03\n",
      " -8.81879963e-03 -1.02337494e-01 -9.14853960e-02 -1.87836111e-01\n",
      "  2.44250894e-01 -1.47913545e-01  6.71698973e-02  5.79364002e-02\n",
      " -2.16445044e-01 -4.42338400e-02  2.02115566e-01  6.05320036e-02\n",
      " -1.05292283e-01 -1.60389155e-01  1.05685689e-01  1.44574702e-01\n",
      " -9.49916989e-02 -6.15625009e-02 -9.73447934e-02 -1.56234398e-01\n",
      "  7.02364892e-02 -2.72877999e-02  4.17655101e-03  3.05995494e-02\n",
      "  6.60447329e-02  4.81186993e-02 -2.00022012e-02 -1.07157208e-01\n",
      "  2.07515005e-02 -1.11394621e-01  8.48550163e-03  1.19025977e-02\n",
      "  1.13621950e-02 -1.49501383e-01 -3.51001501e-01  5.64678013e-02\n",
      "  1.21944621e-01 -1.11272290e-01 -4.45610052e-03  8.85736048e-02\n",
      "  1.83467977e-02 -1.52556002e-02 -3.79473381e-02  1.02317795e-01\n",
      "  1.35775715e-01 -3.69632989e-02 -2.79254317e-02  9.70505998e-02\n",
      "  8.52179900e-02  2.01948807e-01  6.10720227e-03  5.89480996e-02\n",
      "  3.87830883e-02 -3.99859138e-02 -1.39068693e-01  2.63231993e-01\n",
      " -2.48654000e-02  9.57983509e-02  3.91895184e-03  1.10756114e-01\n",
      "  4.95524928e-02 -1.43819809e-01 -1.28051996e-01 -1.09742284e-02\n",
      " -8.77822489e-02  1.28861874e-01  1.55271992e-01 -1.17266700e-01\n",
      " -4.05014977e-02  5.13070002e-02  7.21820742e-02  1.91195399e-01\n",
      "  4.10711989e-02 -7.72915930e-02 -7.08080754e-02 -9.71624777e-02\n",
      "  2.55995810e-01  2.51355767e-01  2.09300518e-02  3.95144001e-02\n",
      "  1.59459114e-01 -3.98249999e-02 -1.97775513e-01  4.47143987e-02\n",
      "  5.23810267e-01  1.64561003e-01 -1.56155974e-01 -1.52225897e-01\n",
      " -1.98557973e-02 -1.72213495e-01 -1.17671505e-01  1.61061198e-01\n",
      " -3.82662937e-02 -6.75619859e-03  1.45996017e-02  2.36894995e-01\n",
      "  1.08408108e-01 -2.75281016e-02 -1.59759820e-03 -1.58994198e-01\n",
      " -4.15528975e-02  1.12410192e-03  1.12073801e-01 -1.08727068e-02\n",
      "  5.05218022e-02  5.98489046e-02 -4.22674939e-02 -1.64209992e-01\n",
      " -2.24625394e-01 -1.67617604e-01  2.45279476e-01 -4.11821008e-02\n",
      " -4.43226956e-02 -1.27027303e-01  1.00868002e-01  1.34983405e-01]\n",
      "Success!\n",
      "text: ['vouchers', 'sleep', 'who', 'some', 'aight', 'little', 'full', 'as', 'leave', 'shopping']\n",
      "Success!\n",
      "text: ['bed', 'use', 'messages', 'my', 'am', 'last', 'morning', 'never', 'cool', 'name']\n",
      "Success!\n",
      "text: ['show', 'wan', 'word', 'wat', 'part', 'on', 'find', '1000', 'apply', 'hello']\n",
      "Success!\n",
      "text: ['beautiful', 'actually', 'gt', 'just', 'full', 'without', 'back', 'sms', '150ppm', 'cost']\n",
      "Success!\n",
      "text: ['speak', 'let', 'around', 'much', 'room', 'yo', 'ok', 'half', 'coming', 'cost']\n",
      "Success!\n",
      "text: ['he', 'word', 'dis', 'heart', 'make', 'while', 'tmr', 'why', 'getting', 'many']\n",
      "Success!\n",
      "text: ['pick', 'dun', 'after', 'music', 'xxx', 'know', 'being', 'big', 'pick', 'service']\n",
      "Success!\n",
      "text: ['gud', 'much', 'didnt', 'next', 'txt', 'ready', 'for', 'want', 'driving', 'friends']\n",
      "Success!\n",
      "text: ['xxx', 'stuff', 'win', 'mobile', 'da', 'to', 'than', 'offer', 'lot', 'guy']\n",
      "Success!\n",
      "text: ['ask', 'without', 'got', 'love', 'always', 'school', 'one', 'those', 'thk', 'able']\n",
      "Success!\n",
      "text: ['pain', 'tell', 'out', 'are', 'guaranteed', 'again', 'person', 'messages', 'years', 'them']\n",
      "Success!\n",
      "text: ['everything', 'weekly', 'sleep', 'why', 'face', 'com', 'say', 'phone', 'next', 'it']\n",
      "Success!\n",
      "text: ['walk', 'ready', 'bed', 'free', 'haven', 'bored', 'hear', 'dreams', 'first', 'over']\n",
      "Success!\n",
      "text: ['big', 'ready', 'once', 'haha', 'got', 'play', 'missed', 'being', 'ya', 'wait']\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# TEST FOR THE get_document_embedding_from_spacy function\n",
    "for test_num in range(15):\n",
    "    sample_token_indices = np.random.randint(0,500, size=10)\n",
    "    input_text = [index_to_token(index, VOCAB) for index in sample_token_indices]\n",
    "    input_text_as_str = \" \".join(input_text)\n",
    "    print(f\"text: {input_text}\")\n",
    "    if test_num == 0:\n",
    "        print(f\"your embedding: {get_document_embedding_from_spacy(input_text_as_str)}\")\n",
    "        print(f\"calculated embedding: {spacy_pipeline(input_text_as_str).vector}\")\n",
    "    assert np.isclose(get_document_embedding_from_spacy(input_text_as_str), spacy_pipeline(input_text_as_str).vector).all()\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wsTeOHpY2g0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression on Word2Vec embedding; accuracy on validation data: 0.9712918660287081\n"
     ]
    }
   ],
   "source": [
    "# This might take around 1 minute to run\n",
    "X_train_spacy = np.vstack([get_document_embedding_from_spacy(x) for x in X_train])\n",
    "X_valid_spacy = np.vstack([get_document_embedding_from_spacy(x) for x in X_val])\n",
    "logreg_w2v = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_spacy, y_train)\n",
    "print(f\" Logistic Regression on Word2Vec embedding; accuracy on validation data: {accuracy_score(y_val , logreg_w2v.predict(X_valid_spacy))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUgPJT303570"
   },
   "source": [
    "### Q2.f.b [2 points] Implement the function get_document_embedding_from_spacy_alt below to return the document embedding based on the token vectors.\n",
    "\n",
    "Feel free to let your imagination run, or go with simplicity. Don't use the average as above. Feel free to play around and see what works and what doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used max which would be like taking the most important token in the text for spam recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4PK2CpdcuOil"
   },
   "outputs": [],
   "source": [
    "def get_document_embedding_from_spacy_alt(text:str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    returns the embedding vector representing the entire text by whatever means you\n",
    "    would like based on the token level embeddings\n",
    "    \"\"\"\n",
    "    tokens = spacy_pipeline(text)\n",
    "\n",
    "    document_embedding = np.max([token.vector for token in tokens], axis=0)\n",
    "\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UrgKFCLjp2ko"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression on Word2Vec alternative embedding; accuracy on validation data: 0.9808612440191388\n"
     ]
    }
   ],
   "source": [
    "# This might take around a 1 minute\n",
    "X_train_spacy_alt = np.vstack([get_document_embedding_from_spacy_alt(x) for x in X_train])\n",
    "X_valid_spacy_alt = np.vstack([get_document_embedding_from_spacy_alt(x) for x in X_val])\n",
    "logreg_w2v_alt = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_spacy_alt, y_train)\n",
    "print(f\" Logistic Regression on Word2Vec alternative embedding; accuracy on validation data: {accuracy_score(y_val , logreg_w2v_alt.predict(X_valid_spacy_alt))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzxBMiHBvaum"
   },
   "source": [
    "## Combining Embeddings [Free]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "eMx3NlAquqMG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression on combined W2V and BoW embeddings; accuracy on validation data: 0.9844497607655502\n"
     ]
    }
   ],
   "source": [
    "X_train_combined = np.hstack((X_train_spacy, vectorized_data_auto.toarray()))\n",
    "X_val_combined = np.hstack((X_valid_spacy, vectorizer_auto.transform(X_val).toarray()))\n",
    "logreg_combined = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_combined, y_train)\n",
    "print(f\" Logistic Regression on combined W2V and BoW embeddings; accuracy on validation data: {logreg_combined.score(X_val_combined, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN4fRAjfxioQ"
   },
   "source": [
    "## [2 points] Q2.g Transformer Model\n",
    "\n",
    "Below we will be using embeddings produced by a \"Transformer\" model (we will learn more about these in week 12/13), for the embedding of each token, it aims to incorporate not only the meaning of the token, but also the specific context in which it occurs here, with the context being a fixed window of size 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fWudJJ2Qxhxn"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\") # This model has been trained on this data\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F6WNlHq57yJ"
   },
   "source": [
    "### Q2.g.a [2 points] Finish the implementation of get_embedding_from_transformer to produce a document level embedding given some text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "u49GVcFtxsV4"
   },
   "outputs": [],
   "source": [
    "def get_embedding_from_transformer(text:str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an embedding from a transformer model produced by averaging the token level embeddings\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer([text])\n",
    "    for key, value in tokenized.items():\n",
    "        tokenized[key] = torch.LongTensor(value)\n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model.forward(**tokenized)[\"hidden_states\"][0].numpy()[0]\n",
    "    # print(token_embeddings.shape)\n",
    "    document_embedding = np.mean(token_embeddings, axis=0)\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "e2zZOpJb6ZNt"
   },
   "outputs": [],
   "source": [
    "# JUST SOME TESTING THAT IT RUNS AND PRODUCES THE RIGHT SIZE\n",
    "for e in [get_embedding_from_transformer(x) for x in X_train[:3]]:\n",
    "    assert e.shape == (128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RQ_EvOXAylxs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression on Transformer embedding accuracy on validation: 0.9844497607655502\n"
     ]
    }
   ],
   "source": [
    "# This might take around a 1 minute\n",
    "X_train_transformer = np.vstack([get_embedding_from_transformer(x) for x in X_train])\n",
    "X_valid_transformer = np.vstack([get_embedding_from_transformer(x) for x in X_val])\n",
    "logreg_transformer = LogisticRegression(random_state=1, max_iter=1000).fit(X_train_transformer, y_train)\n",
    "print(f\" Logistic Regression on Transformer embedding accuracy on validation: {logreg_transformer.score(X_valid_transformer, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLt2Q4iWfeP9"
   },
   "source": [
    "## Q2.h [3 points] Which spam detection method worked best? Why do you think that is? What are some considerations you would need to make to use this model in practice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5LEwqxPpz_P"
   },
   "source": [
    "The transformer model worked best in terms of the accuracy metric. I think that is because it takes into considerations the context in which the word is found, in any language this is very important to know the true meaning of the word. To use this model we would need to consider words that are written the same, homonyms, in orther to avois reaching wrong conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MWyKRONCVFYD"
   ],
   "name": "STA414 Homework 3, Part 2: NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
